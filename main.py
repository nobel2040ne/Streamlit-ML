# JML
# "/Users/jinmyounglee/Desktop/SASA_Sophomore/Sophomore2NDSemester/Interactive Physics/2. Data Science"
import streamlit as st
import pyreadstat
from time import sleep
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import spearmanr, chi2_contingency
import plotly.express as px
import streamlit.components.v1 as components
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import (
    LabelEncoder, OrdinalEncoder,
    StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, HalvingGridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

st.set_page_config(
    page_title="ì£¼ê´€ì  í–‰ë³µ ì¸ì§€ì— ë¯¸ì¹˜ëŠ” ìš”ì¸ ë¶„ì„",
    page_icon="ğŸ€",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.markdown("""
    <style>
    /* ì„ íƒëœ íƒœê·¸ì˜ ë°°ê²½ìƒ‰ ë° í…ìŠ¤íŠ¸ ìƒ‰ìƒ ë³€ê²½ */
    .stMultiSelect div[data-baseweb="tag"] {
        background-color: #f1f1f1 !important; /* ì—°í•œ íšŒìƒ‰ ë°°ê²½ */
        color: #000000 !important; /* ê²€ì • í…ìŠ¤íŠ¸ */
        border: 1px solid #cccccc !important; /* íšŒìƒ‰ í…Œë‘ë¦¬ */
        border-radius: 0.5em; /* ë‘¥ê·¼ ëª¨ì„œë¦¬ */
        padding: 0.2em 0.5em;
    }
    
    /* ì„ íƒëœ íƒœê·¸ì˜ ì‚­ì œ ë²„íŠ¼(X ì•„ì´ì½˜) ìƒ‰ìƒ ë³€ê²½ */
    .stMultiSelect div[data-baseweb="tag"] svg {
        fill: #000000 !important; /* ê²€ì •ìƒ‰ ì•„ì´ì½˜ */
    }

    /* ì„ íƒëœ íƒœê·¸ í˜¸ë²„ ì‹œ ìŠ¤íƒ€ì¼ */
    .stMultiSelect div[data-baseweb="tag"]:hover {
        background-color: #e6e6e6 !important; /* ì¡°ê¸ˆ ë” ì–´ë‘ìš´ íšŒìƒ‰ */
        border-color: #aaaaaa !important; /* í˜¸ë²„ ì‹œ í…Œë‘ë¦¬ ìƒ‰ìƒ */
    }

    /* ë‹¤í¬ ëª¨ ì§€ì› */
    @media (prefers-color-scheme: dark) {
        .stMultiSelect div[data-baseweb="tag"] {
            background-color: #333333 !important; /* ë‹¤í¬ ëª¨ë“œ ë°°ê²½ */
            color: #ffffff !important; /* ë‹¤í¬ ëª¨ë“œ í…ìŠ¤íŠ¸ */
            border: 1px solid #555555 !important; /* ë‹¤í¬ ëª¨ë“œ í…Œë‘ë¦¬ */
        }
        .stMultiSelect div[data-baseweb="tag"]:hover {
            background-color: #444444 !important; /* ë‹¤í¬ ëª¨ë“œ í˜¸ë²„ ë°°ê²½ */
            border-color: #777777 !important;
        }
        .stMultiSelect div[data-baseweb="tag"] svg {
            fill: #ffffff !important; /* ë‹¤í¬ ëª¨ë“œ ì•„ì´ì½˜ í°ìƒ‰ */
        }
    }
    </style>
    """, unsafe_allow_html=True)

plt.rcParams['font.family'] = 'Nanum Gothic'
plt.rcParams['axes.unicode_minus'] = False

components.html("""
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap" rel="stylesheet">
    <style>
    .gradient-text {
        font-family: 'Montserrat', sans-serif;
        font-size: 36px;
        font-weight: bold;
        background: linear-gradient(270deg, #ff7e5f, #feb47b, #86a8e7, #91eae4);
        background-size: 500% 500%;
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        animation: gradientAnimation 10s ease infinite;
    }
    @keyframes gradientAnimation {
        0% { background-position: 0% 50%; }
        50% { background-position: 100% 50%; }
        100% { background-position: 0% 50%; }
    }
    .developer-text {
        font-family: 'Montserrat', sans-serif;
        font-size: 16px;
        font-weight: normal;
        color: #555555;
        margin-top: 10px;
    }            
    </style>
    <div style='text-align: center;'>
        <h1 class="gradient-text">ì£¼ê´€ì  í–‰ë³µ ì¸ì§€ì— ë¯¸ì¹˜ëŠ” ìš”ì¸ ë¶„ì„</h1>
        <p class="developer-text">JML</p>
    </div>
    """, height=120)

@st.cache_data
def load_data(file_path):
    df, meta = pyreadstat.read_sav(file_path)
    return df, meta

file_path = "kyrbs2023.sav"

try:
    df, meta = load_data(file_path)    
except FileNotFoundError:
    st.error(f"File Not Found: {file_path}")
    st.stop()
except Exception as e:
    st.error(f"Error Loading Data: {e}")
    st.stop()    

columns_X = ['PR_HT', 'M_SUI_CON', 'M_STR', 'M_SLP_EN', 'M_SAD', 'M_LON', 'M_GAD_1', 'M_GAD_2', 'M_GAD_3',
             'M_GAD_4', 'M_GAD_5', 'M_GAD_6', 'M_GAD_7', 'PA_MSC', 'PA_TOT', 'PA_VIG_D']
columns_Y = ['PR_HD']

column_mapping = {
    'PR_HT' : 'ì£¼ê´€ì  ê±´ê°• ì¸ì§€',
    'PR_HD' : 'ì£¼ê´€ì  í–‰ë³µ ì¸ì§€',
    'M_STR': 'í‰ìƒì‹œ ìŠ¤íŠ¸ë ˆìŠ¤ ì¸ì§€',  # 1 ë§ì´ ~ 5 ì „í˜€
    'M_SLP_EN': 'ì ìœ¼ë¡œ í”¼ë¡œíšŒë³µ ì •ë„',  # 1 ì¶©ë¶„ ~ 5 ë¶€ì¡±
    'M_SAD': 'ìŠ¬í”” ì ˆë§ê° ê²½í—˜',  # 1: ì—†ìŒ, 2: ìˆìŒ
    'M_SUI_CON': 'ìì‚´ ìƒê°',
    'M_LON': 'ì™¸ë¡œì›€ ê²½í—˜',  # 1: ì—†ìŒ ~ 5: í•­ìƒ
    'M_GAD_1': 'ì´ˆì¡°í•¨ ë° ë¶ˆì•ˆí•¨',  # 1: ì „í˜€ ~ 5: ì¼
    'M_GAD_2': 'ê±±ì • ë©ˆì¶œ ìˆ˜ ì—†ìŒ',
    'M_GAD_3': 'ê±±ì • ë„ˆë¬´ ë§ìŒ',
    'M_GAD_4': 'í¸í•˜ê²Œ ìˆê¸° ì–´ë ¤ì›€',
    'M_GAD_5': 'ë„ˆë¬´ ì•ˆì ˆë¶€ì ˆ ëª»í•¨',
    'M_GAD_6': 'ì‰½ê²Œ ì§œì¦ì´ ë‚¨',
    'M_GAD_7': 'ë”ì°í•œ ì¼ì´ ìƒê¸¸ ê²ƒ ê°™ìŒ',
    'PA_MSC': 'ê·¼ë ¥ ê°•í™” ìš´ë™ ì¼ìˆ˜',
    'PA_TOT': 'í•˜ë£¨ 60ë¶„ ì´ìƒ ì‹ ì²´í™œë™ ì¼ìˆ˜',
    'PA_VIG_D' : 'ê³ ê°•ë„ ì‹ ì²´í™œë™ ì¼ìˆ˜'
}

variable_type_mapping = {
    'PR_HT' : 'continuous',
    'PR_HD' : 'categorical', 
    'M_STR' : 'continuous', 
    'M_SLP_EN' : 'continuous', 
    'M_SAD' : 'categorical', 
    'M_SUI_CON' : 'categorical', 
    'M_LON' : 'continuous', 
    'M_GAD_1' : 'continuous', 
    'M_GAD_2' : 'continuous',
    'M_GAD_3' : 'continuous', 
    'M_GAD_4' : 'continuous', 
    'M_GAD_5' : 'continuous', 
    'M_GAD_6' : 'continuous', 
    'M_GAD_7' : 'continuous',
    'PA_TOT' : 'continuous',
    'PA_VIG_D' : 'continuous',
    'PA_MSC' : 'continuous'
}

tab1, tab2 = st.tabs(["ğŸ“š EDA", "ğŸ¸ ML"])

with tab1:
    # ê²°ì¸¡ì¹˜ ë° ì´ìƒì¹˜ ì²˜ë¦¬ ì„¤ì •
    missing_value_options = [
        "No handling",
        "Drop rows with missing values",
        "Impute with mean",
        "Impute with median",
        "Impute with mode"
    ]

    outlier_handling_options = [
        "No outlier handling",
        "Remove outliers using Z-score",
        "Remove outliers using IQR"
    ]

    selected_missing_value_method = st.selectbox(
        "Select method for handling missing values:",
        missing_value_options,
        key="missing_value_method"
    )

    selected_outlier_method = st.selectbox(
        "Select method for handling outliers:",
        outlier_handling_options,
        key="outlier_handling_method"
    )

    # If Z-score method is selected, allow user to set Z-score threshold
    if selected_outlier_method == "Remove outliers using Z-score":
        z_threshold = st.number_input(
            "Set Z-score threshold:",
            min_value=2.0,
            value=3.0,
            step=0.1,
            key="z_score_threshold"
        )

    # If IQR method is selected, allow user to set IQR multiplier
    if selected_outlier_method == "Remove outliers using IQR":
        iqr_multiplier = st.number_input(
            "Set IQR multiplier:",
            min_value=0.0,
            value=1.5,  # ê¸°ë³¸ê°’ ìœ ì§€
            step=0.1,
            key="iqr_multiplier"
        )

    if st.button("Apply"):
        # df_selectedëŠ” ì´ë¯¸ ì„ íƒëœ columns_X, columns_Yë¡œ êµ¬ì„±í•œ ë°ì´í„°í”„ë ˆì„ì´ë¼ê³  ê°€ì •
        df_selected_cleaned = df[columns_X + columns_Y].copy()

        # 1. Handling Missing Values
        if selected_missing_value_method == "Drop rows with missing values":
            df_selected_cleaned = df_selected_cleaned.dropna()
            st.write("Dropped rows with missing values.")
        elif selected_missing_value_method == "Impute with mean":
            numeric_cols = df_selected_cleaned.select_dtypes(include=[np.number]).columns.tolist()
            df_selected_cleaned[numeric_cols] = df_selected_cleaned[numeric_cols].fillna(df_selected_cleaned[numeric_cols].mean())
            st.write("Imputed missing values with mean.")
        elif selected_missing_value_method == "Impute with median":
            numeric_cols = df_selected_cleaned.select_dtypes(include=[np.number]).columns.tolist()
            df_selected_cleaned[numeric_cols] = df_selected_cleaned[numeric_cols].fillna(df_selected_cleaned[numeric_cols].median())
            st.write("Imputed missing values with median.")
        elif selected_missing_value_method == "Impute with mode":
            # ë²”ì£¼í˜• ë³€ìˆ˜ì— ëŒ€í•œ ëŒ€ì¹˜
            for col in df_selected_cleaned.columns:
                if df_selected_cleaned[col].isnull().sum() > 0:
                    mode_val = df_selected_cleaned[col].mode()
                    if len(mode_val) > 0:
                        df_selected_cleaned[col].fillna(mode_val[0], inplace=True)
            st.write("Imputed missing values with mode.")
        else:
            st.write("No missing value handling applied.")

        # 2. Handling Outliers
        if selected_outlier_method == "Remove outliers using Z-score":
            numeric_cols = df_selected_cleaned.select_dtypes(include=[np.number]).columns.tolist()
            if numeric_cols:
                z_scores = np.abs(stats.zscore(df_selected_cleaned[numeric_cols]))
                outliers = (z_scores > z_threshold).any(axis=1)
                removed_rows = outliers.sum()
                df_selected_cleaned = df_selected_cleaned[~outliers]
                st.write(f"Removed {removed_rows} rows using Z-score method with threshold {z_threshold}.")
            else:
                st.write("No numeric columns available for Z-score outlier removal.")
        elif selected_outlier_method == "Remove outliers using IQR":
            numeric_cols = df_selected_cleaned.select_dtypes(include=[np.number]).columns.tolist()
            if numeric_cols:
                Q1 = df_selected_cleaned[numeric_cols].quantile(0.25)
                Q3 = df_selected_cleaned[numeric_cols].quantile(0.75)
                IQR = Q3 - Q1
                outliers = ((df_selected_cleaned[numeric_cols] < (Q1 - iqr_multiplier * IQR)) | 
                            (df_selected_cleaned[numeric_cols] > (Q3 + iqr_multiplier * IQR))).any(axis=1)
                removed_rows = outliers.sum()
                df_selected_cleaned = df_selected_cleaned[~outliers]
                st.write(f"Removed {removed_rows} rows using IQR method with multiplier {iqr_multiplier}.")
            else:
                st.write("No numeric columns available for IQR outlier removal.")
        else:
            st.write("No outlier handling applied.")

        st.write("**ê²°ì¸¡ì¹˜ ë° ì´ìƒì¹˜ ì²˜ë¦¬ ì™„ë£Œ**")
        st.write("Data Preview after cleaning:", df_selected_cleaned.head())

        # íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
        numeric_cols = df_selected_cleaned.select_dtypes(include=['int64', 'float64']).columns.tolist()

        if len(numeric_cols) < 2:
            st.write("ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•  ì¶©ë¶„í•œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            df_numeric = df_selected_cleaned[numeric_cols]
            corr = df_numeric.corr(method='spearman').fillna(0)
            labels = [column_mapping.get(col, col) for col in corr.columns]

            fig_corr = px.imshow(
                corr.values,
                x=labels,
                y=labels,
                aspect="auto",
                color_continuous_scale='RdBu_r',
                zmin=-1,
                zmax=1,
                width=600,
                height=600,
                text_auto=True
            )

            fig_corr.update_layout(
                xaxis=dict(tickfont=dict(size=10)),
                yaxis=dict(tickfont=dict(size=10)),
                title=dict(text='Overall HEATMAP (Ignoring the Variable Type)', font=dict(size=20)),
                autosize=False,
                margin=dict(l=100, r=100, t=100, b=100)
            )

            st.plotly_chart(fig_corr)

    # ì„ íƒ ìƒìì— ë¼ë²¨ í‘œì‹œ
    col1, col2 = st.columns(2)

    # ë§Œì•½ columns_X, columns_Yê°€ ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆë‹¤ë©´
    selected_X = columns_X
    selected_Y = columns_Y

    # ì„ íƒëœ ë³€ìˆ˜ë“¤(selected_X, selected_Y) ì´í›„ì— ë‹¤ìŒê³¼ ê°™ì´ ì¶”ê°€:
    columns_X_labels = [column_mapping.get(col, col) for col in selected_X]
    columns_Y_labels = [column_mapping.get(col, col) for col in selected_Y]

    label_to_column = {label: col for col, label in zip(selected_X + selected_Y, columns_X_labels + columns_Y_labels)}

    # ì´í›„ì— x_axis_label, y_axis_label ì„ íƒë°•ìŠ¤ ì½”ë“œ
    col1, col2 = st.columns(2)

    with col1:
        x_axis_label = st.selectbox("Xì¶• ë³€ìˆ˜ ì„ íƒ", columns_X_labels)
    with col2:
        y_axis_label = st.selectbox("Yì¶• ë³€ìˆ˜ ì„ íƒ", columns_Y_labels)

    x_axis = label_to_column[x_axis_label]
    y_axis = label_to_column[y_axis_label]

    # Determine the types of the selected variables
    x_type = variable_type_mapping.get(x_axis, 'categorical')
    y_type = variable_type_mapping.get(y_axis, 'categorical')

    st.markdown(f"**ì„ íƒí•œ ë³€ìˆ˜ ìœ í˜•:**")
    st.markdown(f"- **Xì¶• ({x_axis_label})**: **{x_type}**")
    st.markdown(f"- **Yì¶• ({y_axis_label})**: **{y_type}**")

    def cramers_v(confusion_matrix):
        """
        Calculate CramÃ©r's V for a confusion matrix.

        Parameters:
        - confusion_matrix (pd.DataFrame): Cross-tabulation table.

        Returns:
        - float: CramÃ©r's V value.
        """
        chi2, p, dof, expected = chi2_contingency(confusion_matrix)
        n = confusion_matrix.sum().sum()
        phi2 = chi2 / n
        r, k = confusion_matrix.shape
        # Adjust for bias
        phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))
        rcorr = r - ((r-1)**2)/(n-1)
        kcorr = k - ((k-1)**2)/(n-1)
        return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))

    # Visualization based on variable types
    if x_type == 'categorical' and y_type == 'categorical':
        st.subheader("Cross-Tabulation")
        # ê°€ìš´ë° ì •ë ¬ì„ ìœ„í•œ ì»¬ëŸ¼ ìƒì„±
        col_left, col_center, col_right = st.columns([1, 2, 1])

        with col_center:
            cross_tab = pd.crosstab(df[x_axis], df[y_axis])
            cross_tab = cross_tab.rename(index=column_mapping, columns=column_mapping)
            st.dataframe(cross_tab)

        # ì¹´ì´ì œê³± ê²€ì • ë° í¬ë˜ë¨¸ì˜ V ê³„ì‚°
        cross_tab = pd.crosstab(df[x_axis], df[y_axis])
        chi2, p, dof, expected = chi2_contingency(cross_tab)
        cramers_v_value = cramers_v(cross_tab)

        # ê²°ê³¼ í‘œì‹œ with gradient styling in 2x2 grid
        col1, col2 = st.columns(2)
        col3, col4 = st.columns(2)

        with col1:
            st.markdown(f"""
            <div style="text-align: center;">
                <p style="
                    font-size: 24px;
                    font-weight: bold;
                    background: linear-gradient(90deg, #ff7f50, #1e90ff);
                    -webkit-background-clip: text;
                    -webkit-text-fill-color: transparent;
                    margin: 0;
                ">
                    Chi-Squared Test: {chi2:.3f}
                </p>
            </div>
            """, unsafe_allow_html=True)

        with col2:
            st.markdown(f"""
            <div style="text-align: center;">
                <p style="
                    font-size: 24px;
                    font-weight: bold;
                    background: linear-gradient(90deg, #1e90ff, #ff7f50);
                    -webkit-background-clip: text;
                    -webkit-text-fill-color: transparent;
                    margin: 0;
                ">
                    Degree of Freedom: {dof}
                </p>
            </div>
            """, unsafe_allow_html=True)

        with col3:
            st.markdown(f"""
            <div style="text-align: center;">
                <p style="
                    font-size: 24px;
                    font-weight: bold;
                    background: linear-gradient(90deg, #ff7f50, #1e90ff);
                    -webkit-background-clip: text;
                    -webkit-text-fill-color: transparent;
                    margin: 0;
                ">
                    p-value: {p:.3e}
                </p>
            </div>
            """, unsafe_allow_html=True)

        with col4:
            st.markdown(f"""
            <div style="text-align: center;">
                <p style="
                    font-size: 24px;
                    font-weight: bold;
                    background: linear-gradient(90deg, #1e90ff, #ff7f50);
                    -webkit-background-clip: text;
                    -webkit-text-fill-color: transparent;
                    margin: 0;
                ">
                    CramÃ©r's V: {cramers_v_value:.3f}
                </p>
            </div>
            """, unsafe_allow_html=True)

    elif x_type == 'continuous' and y_type == 'continuous':
        st.subheader("Heatmap")
        # Calculate correlation matrix
        corr_matrix = df[[x_axis, y_axis]].corr(method='spearman')
        # Plot heatmap
        fig = px.imshow(
            corr_matrix,
            x=[column_mapping.get(x_axis, x_axis), column_mapping.get(y_axis, y_axis)],
            y=[column_mapping.get(x_axis, x_axis), column_mapping.get(y_axis, y_axis)],
            color_continuous_scale='RdBu_r',
            zmin=-1,
            zmax=1,
            text_auto=True,
            width=600,
            height=600
        )
        st.plotly_chart(fig)

        # ê³„ì‚°ëœ Spearman ìƒê´€ê³„ ë° p-value
        x_data = df[x_axis]
        y_data = df[y_axis]

        # ë°ì´í„° íƒ€ì…ì´ ìˆ˜ì¹˜í˜•ì¸ì§€ í™•ì¸
        if x_data.dtype.kind in 'biufc' and y_data.dtype.kind in 'biufc':
            corr_coef, p_value = spearmanr(x_data, y_data)

            st.markdown(f"""
             <div style="text-align: center;">
                <p style="
                    font-size: 24px;
                    font-weight: bold;
                    background: linear-gradient(90deg, #ff7f50, #1e90ff);
                    -webkit-background-clip: text;
                    -webkit-text-fill-color: transparent;
                    margin: 0;
                ">
                    Correlation Coefficient: {corr_coef:.3f}
                </p>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.write("ì„ íƒí•œ ë³€ìˆ˜ëŠ” ìˆ˜ì¹˜í˜• ë°ì´í„°ê°€ ì•„ë‹ˆì–´ì„œ ìƒê´€ê³„ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    elif (x_type == 'categorical' and y_type == 'continuous') or (x_type == 'continuous' and y_type == 'categorical'):
        st.subheader("Bar Graph")

        # Determine which is categorical and which is continuous
        if x_type == 'categorical':
            cat_var = x_axis
            cat_label = x_axis_label
            cont_var = y_axis
            cont_label = y_axis_label
        else:
            cat_var = y_axis
            cat_label = y_axis_label
            cont_var = x_axis
            cont_label = x_axis_label

        # Aggregate data
        agg_df = df.groupby(cat_var)[cont_var].mean().reset_index()
        agg_df = agg_df.rename(columns={
            cat_var: cat_label,
            cont_var: f'í‰ê·  {cont_label}'
        })

        # Plot bar chart
        fig = px.bar(
            agg_df,
            x=cat_label,
            y=f'í‰ê·  {cont_label}',
            labels={f'í‰ê·  {cont_label}': f'í‰ê·  {cont_label}'},
            title=f'{cat_label}ì— ë”°ë¥¸ í‰ê·  {cont_label}'
        )
        st.plotly_chart(fig)

    else:
        st.error("ìœ íš¨í•œ ë³€ìˆ˜ ìœ í˜• ì¡°í•©ì´ ì•„ë‹™ë‹ˆë‹¤.")

    st.write("")
    st.write("")

    st.write("## Overall")

    st.markdown("""
        <style>
        /* ìŠ¬ë¼ì´ë” íŠ¸ë™ì˜ ë‘ê»˜ ì¡°ì ˆ */
        div[data-baseweb="slider"] > div > div {
            height: 20px;
        }

        /* ìŠ¬ë¼ì´ë” ê°’ì˜ ìœ„ì¹˜ ì¡°ì • */
        div[data-baseweb="slider"] > div > div > div > div {
            top: -30px !important;
            font-size: 15px !important;
            color: #fff !important;
        }
        </style>
        """, unsafe_allow_html=True)

    numeric_df = df.select_dtypes(include=['float64', 'int64'])

    if len(numeric_df.columns) < 2:
        st.write("ìƒê´€ê´€ê³„ íˆíŠ¸ë§µì„ ê·¸ë¦´ ì¶©ë¶„í•œ ìˆ«ìí˜• ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        col1, col2 = st.columns(2)

        with col1:
            # ì¢…ì† ë³€ìˆ˜ ì„ íƒ (ë¼ë²¨ë¡œ í‘œì‹œ)
            dependent_var_label = st.selectbox(
            "ì¢…ì† ë³€ìˆ˜ ì„ íƒ",
            [column_mapping.get(col, col) for col in numeric_df.columns if col != 'ì£¼ê´€ì  í–‰ë³µ ì¸ì§€'],
            index=[column_mapping.get(col, col) for col in numeric_df.columns if col != 'ì£¼ê´€ì  í–‰ë³µ ì¸ì§€'].index('ì£¼ê´€ì  í–‰ë³µ ì¸ì§€')
            )

        with col2:
            threshold = st.slider("ìƒê´€ ê³„ìˆ˜ ì„ê³„ê°’ ì„¤ì •", min_value=0.0, max_value=1.0, value=0.3, step=0.01)

        # ë¼ë²¨ì„ ì»¬ëŸ¼ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
        dependent_var = label_to_column.get(dependent_var_label, dependent_var_label)

        correlations = numeric_df.corr()

        filtered_correlations = correlations[[dependent_var]][(correlations[dependent_var].abs() >= threshold)
                                                               & (correlations.index != dependent_var)]
    
        if filtered_correlations.empty:
            st.write(f"{dependent_var_label}ì™€ ìƒê´€ ê³„ìˆ˜ {threshold} ì´ìƒì¸ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # í•„í„°ë§ëœ ìƒ ê³„ìˆ˜ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ (ë¼ë²¨ë¡œ í‘œì‹œ)
            filtered_correlations['abs_corr'] = filtered_correlations[dependent_var].abs()
            filtered_correlations = filtered_correlations.sort_values(by='abs_corr', ascending=False).drop(columns=['abs_corr'])
    
            filtered_correlations.index = [column_mapping.get(col, col) for col in filtered_correlations.index]

            st.write(f"**{dependent_var_label}**ì™€ ìƒê´€ ê³„ìˆ˜ **{threshold}** ì´ìƒì¸ ë³€ìˆ˜:")
            st.dataframe(filtered_correlations, use_container_width=True)

    st.write("Data Preview", df.head())

    # ëª¨ë“  ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì„ íƒ
    numeric_cols = numeric_df.columns.tolist()

    if len(numeric_cols) < 2:
        st.write("ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•  ì¶©ë¶„í•œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ìƒê´€ê³„ìˆ˜ ê³„
        corr_matrix = numeric_df.corr().abs()  # ì ˆëŒ“ê°’ ìš©

        # ì¤‘ë³µ ë° ìê¸° ìì‹ ì„ ì œì™¸í•œ ìƒê° í–‰ë ¬ì˜ ì¸ë± ì¶”ì¶œ
        corr_pairs = (
            corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
            .stack()
            .reset_index()
        )
        corr_pairs.columns = ['ë³€ìˆ˜1', 'ë³€ìˆ˜2', 'ìƒê´€ê³„ìˆ˜']

        # ìƒê´€ê³„ìˆ˜ê°€ 0.4 ì´ìƒì¸ ë³€ìˆ˜ ìŒ í•„í„°ë§
        threshold = 0.4
        strong_corr_pairs = corr_pairs[corr_pairs['ìƒê´€ê³„ìˆ˜'] >= threshold]

        if strong_corr_pairs.empty:
            st.write(f"ìƒê´€ê³„ìˆ˜ê°€ {threshold} ì´ìƒì¸ ë³€ìˆ˜ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ë³€ìˆ˜ ì´ë¦„ì„ ë¼ë²¨ë¡œ ë³€í™˜
            strong_corr_pairs['ë³€ìˆ˜1'] = strong_corr_pairs['ë³€ìˆ˜1'].apply(lambda x: column_mapping.get(x, x))
            strong_corr_pairs['ë³€ìˆ˜2'] = strong_corr_pairs['ë³€ìˆ˜2'].apply(lambda x: column_mapping.get(x, x))

            # ìƒê´€ê³„ìˆ˜ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            strong_corr_pairs = strong_corr_pairs.sort_values(by='ìƒê´€ê³„ìˆ˜', ascending=False)

            st.write(f"ìƒê´€ê³„ìˆ˜ê°€ {threshold} ì´ìƒì¸ ë³€ìˆ˜ ìŒ")
            st.dataframe(strong_corr_pairs.reset_index(drop=True), use_container_width=True)

with tab2:
    st.write("âš ï¸ EDAì—ì„œ ì‘ì—…í•˜ì˜€ë‹¤ë©´, ìƒˆë¡œê³ ì¹¨ ì´í›„ ë‹¤ì‹œ ì ‘ì†í•˜ì„¸ìš”")
    # Initialize session state keys if they do not exist
    if 'selected_Y' not in st.session_state:
        st.session_state['selected_Y'] = []  # or set it to a default value if needed

    # ë‹¨ê³„ ìƒíƒœë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ì´ˆê¸° ì„¤ì •
    if 'step' not in st.session_state:
        st.session_state['step'] = 0

    # ìŠ¤í¬ë¡¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    def scroll_to(section_id):
        st.components.v1.html(f"""
            <script>
                var element = document.getElementById('{section_id}');
                if (element) {{
                    element.scrollIntoView({{behavior: 'smooth'}});
                }}
            </script>
            """, height=0)

    # 0ê³„: ë³€ìˆ˜ íƒ
    st.markdown('<div id="variable_selection"></div>', unsafe_allow_html=True)
    st.subheader("ğŸ“‚ Variable Selection")

    # Combine columns_X and columns_Y to form the initial list of variables
    available_variables = [column_mapping.get(col, col) for col in list(column_mapping.keys())]

    group_1 = columns_X[:6]
    group_2 = columns_X[6:12]
    group_3 = columns_X[12:]

    colX1, colX2, colX3 = st.columns(3)

    with colX1:
        selected_X_labels_1 = st.multiselect(
            "**M ì •ì‹ ê±´ê°•**",
            options=[column_mapping.get(col, col) for col in group_1],
            default=[column_mapping.get(col, col) for col in group_1]
        )

    with colX2:
        selected_X_labels_2 = st.multiselect(
            "**GAD-7 ë²”ë¶ˆì•ˆì¥ì•  ê²½í—˜ ì¡°ì‚¬ë„êµ¬**",
            options=[column_mapping.get(col, col) for col in group_2],
            default=[column_mapping.get(col, col) for col in group_2]
        )

    with colX3:
        selected_X_labels_3 = st.multiselect(
            "**PA ì‹ ì²´í™œë™**",
            options=[column_mapping.get(col, col) for col in group_3],
            default=[column_mapping.get(col, col) for col in group_3]
        )

    # ìœ„ ì„¸ ê·¸ë£¹ì—ì„œ ì„ íƒëœ X ë³€ìˆ˜ ë¼ë²¨ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì¹œë‹¤.
    selected_X_labels = selected_X_labels_1 + selected_X_labels_2 + selected_X_labels_3
    st.markdown("""
        > **ì£¼ê´€ì  í–‰ë³µ ì¸ì§€?** \\
        > âš ï¸ ì£¼ê´€ì  í–‰ë³µ ì¸ì§€ëŠ” 1(1, 2, 3)ê³¼ 2(4, 5)ë¡œ ì´ì§„ ë¶„ë¥˜í•©ë‹ˆë‹¤\\
        > \\
        > í‰ìƒì‹œ ì–¼ë§ˆë‚˜ í–‰ë³µí•˜ë‹¤ê³  ìƒê°í•©ë‹ˆê¹Œ? 
        > > 1 ë§¤ìš° í–‰ë³µí•œ í¸ì´ë‹¤\\
        > > 2 ì•½ê°„ í–‰ë³µí•œ í¸ì´ë‹¤\\
        > > 3 ë³´í†µì´ë‹¤\\
        > > 4 ì•½ê°„ ë¶ˆí–‰í•œ í¸ì´ë‹¤\\
        > > 5 ë§¤ìš° ë¶ˆí–‰í•œ í¸ì´ë‹¤
        """)
    selected_Y_labels = st.multiselect(
        "Select target variables (Y):",
        options=available_variables,
        default=[column_mapping.get(col, col) for col in columns_Y],
        disabled=True
    )

    # ì—­ë§¤í•‘: í•œê¸€ ì´ë¦„ ì›ë˜ ë³€ìˆ˜ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
    selected_X = [
        next((key for key, value in column_mapping.items() if value == label), label)
        for label in selected_X_labels
    ]
    selected_Y = [
        next((key for key, value in column_mapping.items() if value == label), label)
        for label in selected_Y_labels
    ]

    df[selected_Y[0]] = df[selected_Y[0]].apply(lambda x: 2 if x in [4, 5] else 1)        

    # ë°ì´í„°í”„ë ˆì„ ì„œë¸Œì…‹ ìƒì„±
    columns_to_use = list(set(selected_X + selected_Y))

    missing_columns = [col for col in columns_to_use if col not in df.columns]
    if missing_columns:
        st.error(f"The following columns are not in the dataset: {missing_columns}")
        st.stop()

    # ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ ì‘ì—…
    df_subset = df[columns_to_use]

    # ë³€ìˆ˜ ì„ íƒ í›„ 'Next' ë²„íŠ¼
    if st.button("Next", key="next_variable_selection"):
        if not selected_X or not selected_Y:
            st.error("Please select at least one feature and one target variable.")
        else:
            st.session_state['step'] = 1
            st.session_state['df'] = df_subset.copy()
            st.session_state['selected_X'] = selected_X
            st.session_state['selected_Y'] = selected_Y
            st.session_state['column_mapping'] = column_mapping
            st.session_state['variable_type_mapping'] = variable_type_mapping
            scroll_to('dataset_overview')

    if st.session_state.get('step', 0) >= 1:
        st.markdown('<div id="dataset_overview"></div>', unsafe_allow_html=True)
        st.subheader("ğŸ¿ Dataset Overview")
        df = st.session_state['df']
        st.write("Shape of the dataset:", df.shape)
        st.write(df.head())
        st.write(df.describe())

        if st.button("Next", key="next_dataset_overview"):
            st.session_state['step'] = 2
            scroll_to('handling_missing_values')

    if st.session_state.get('step', 0) >= 2:
        st.markdown('<div id="handling_missing_values"></div>', unsafe_allow_html=True)
        st.subheader("ğŸ˜¶ Handling Missing Values and Outliers")

        st.markdown("""
        > **Z-Score?** \\
        > ë°ì´í„° ê°’ì´ í‰ê· ìœ¼ë¡œë¶€í„° ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ë¥¼ í‘œì¤€í¸ì°¨ ë‹¨ìœ„ë¡œ ë‚˜íƒ€ë‚¸ ê°’ì…ë‹ˆë‹¤. \\
        > âš ï¸ Z-Score TresholdëŠ” 3.00ì„ ì¶”ì²œí•©ë‹ˆë‹¤. ë„ˆë¬´ ì‘ì„ ê²½ìš° ì—ëŸ¬ê°€ ë‚˜ê±°ë‚˜ í•™ìŠµì´ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤. \\
        > **IQR?** (Interquartile Range) \\
        > ë°ì´í„°ì˜ ì¤‘ì•™ 50%ë¥¼ í¬í•¨í•˜ëŠ” ë²”ìœ„ë¡œ, Q3(3ë¶„ìœ„ìˆ˜)ì—ì„œ Q1(1ë¶„ìœ„ìˆ˜)ë¥¼ ëº€ ê°’ì…ë‹ˆë‹¤.
        """)

        missing_value_options = [
            "Drop rows with missing values",
            "Impute with mean",
            "Impute with median",
            "Impute with mode"
        ]

        outlier_handling_options = [
            "Remove outliers using Z-score",
            "No outlier handling",
            "Remove outliers using IQR"
        ]

        selected_missing_value_method = st.selectbox(
            "Select method for handling missing values:",
            missing_value_options
        )

        selected_outlier_method = st.selectbox(
            "Select method for handling outliers:",
            outlier_handling_options
        )

        if selected_outlier_method == "Remove outliers using Z-score":
            z_threshold = st.number_input(
                "Set Z-score threshold:",
                min_value=3.0,
                value=3.0,
                step=0.1
            )

        if selected_outlier_method == "Remove outliers using IQR":
            iqr_multiplier = st.number_input(
                "Set IQR multiplier:",
                min_value=0.0,
                value=1.5,
                step=0.1
            )

        if st.button("Apply", key="apply_missing_outliers"):
            df = st.session_state['df'].copy()  # ë³¸ ë°ì´í„° ë³µì‚¬

            initial_shape = df.shape  # ì´ˆê¸° ë°ì´í„° í˜•íƒœ ì €ì¥

            # ### 1. Handling Missing Values ###
            if selected_missing_value_method == "Drop rows with missing values":
                df = df.dropna()
                st.write("Dropped rows with missing values.")
            elif selected_missing_value_method == "Impute with mean":
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())
                st.write("Imputed missing values with mean.")
            elif selected_missing_value_method == "Impute with median":
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
                st.write("Imputed missing values with median.")
            elif selected_missing_value_method == "Impute with mode":
                categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
                for col in categorical_cols:
                    if df[col].isnull().sum() > 0:
                        df[col].fillna(df[col].mode()[0], inplace=True)
                st.write("Imputed missing values with mode.")

            after_missing_shape = df.shape  # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„ ë°ì´í„° í˜•íƒœ ì €ì¥
            st.write(f"Rows before handling missing values: {initial_shape[0]}, after: {after_missing_shape[0]}")

            # ### 2. Handling Outliers ###
            rows_before_outliers = df.shape[0]

            if selected_outlier_method == "Remove outliers using Z-score":
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                if numeric_cols:
                    z_scores = np.abs(stats.zscore(df[numeric_cols]))
                    # Z-ì ìˆ˜ê°€ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ì—¬ë¶€ë¥¼ í™•ì¸ (í•˜ë‚˜ì˜ ì»¬ëŸ¼ì´ë¼ë„ ì´ˆê³¼ë©´ True)
                    outliers = (z_scores > z_threshold).any(axis=1)
                    removed_rows = outliers.sum()
                    df = df[~outliers]
                    st.write(f"Removed {removed_rows} rows using Z-score method with threshold {z_threshold}.")
                else:
                    st.write("No numeric columns available for Z-score outlier removal.")
            elif selected_outlier_method == "Remove outliers using IQR":
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                if numeric_cols:
                    Q1 = df[numeric_cols].quantile(0.25)
                    Q3 = df[numeric_cols].quantile(0.75)
                    IQR = Q3 - Q1
                    # IQRì„ ì´ìš©í•´ ì´ìƒ ì—¬ë¶€ íŒë‹¨ (í•˜ë‚˜ì˜ ì»¬ëŸ¼ë¼ë„ ì´ìƒì¹˜ë©´ True)
                    outliers = ((df[numeric_cols] < (Q1 - iqr_multiplier * IQR)) | (df[numeric_cols] > (Q3 + iqr_multiplier * IQR))).any(axis=1)
                    removed_rows = outliers.sum()
                    df = df[~outliers]
                    st.write(f"Removed {removed_rows} rows using IQR method with multiplier {iqr_multiplier}.")
                else:
                    st.write("No numeric columns available for IQR outlier removal.")
            else:
                st.write("No outlier handling applied.")

            after_outliers_shape = df.shape  # ì´ìƒì¹˜ ì²˜ë¦¬ í›„ ë°ì´í„° í˜•íƒœ ì €ì¥
            st.write(f"Rows before handling outliers: {rows_before_outliers}, after: {after_outliers_shape}")

            # ### 3. Summary ###
            total_removed = initial_shape[0] - after_missing_shape[0] + rows_before_outliers - after_outliers_shape[0]
            st.write(f"Total rows removed: {total_removed}")

            # Update the dataframe in session state
            st.session_state['df'] = df

            st.session_state['step'] = 3
            scroll_to('encoding_categorical_features')

    if st.session_state.get('step', 0) >= 3:
        st.markdown('<div id="encoding_categorical_features"></div>', unsafe_allow_html=True)
        st.subheader("ğŸ² Encoding Categorical Features")
        st.markdown('**âš ï¸ No Encoding Recommended**')

        st.markdown("""
        > **One-Hot Encoding?** \\
        > ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ê³ ìœ í•œ ê°’ë§ˆë‹¤ ì´ì§„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ê° ë²”ì£¼ë¥¼ ë…ë¦½ì ì¸ ì—´ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. \\
        > **Ordinal Encoding?**\\
        > ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìˆœì„œê°€ ìˆëŠ” ì •ìˆ˜ ê°’ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ ë°ì´í„°ì˜ ìˆœì„œ ì •ë³´ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\\
        > **Label Encoding?** \\
        > ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ê³ ìœ í•œ ì •ìˆ˜ ê°’ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ ê°„ê²°í•˜ê²Œ í‘œí˜„í•©ë‹ˆë‹¤. \\
        > **Count Frequency Encoding?** \\
        > ê° ë²”ì£¼ì˜ ë“±ì¥ íšŸìˆ˜ë‚˜ ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜í˜• ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """)
        # Options for encoding categorical variables, including "No Encoding"
        encoding_options = [
            "No Encoding",  # ì¶”ê°€ëœ ì˜µì…˜
            "One Hot Encoding",
            "Ordinal Encoding",
            "Label Encoding",
            "Count Frequency Encoding"
        ]

        # User selection
        selected_encoding_method = st.selectbox(
            "Select encoding method for categorical features:",
            encoding_options
        )

        # Apply button
        if st.button("Apply Encoding"):
            df = st.session_state['df']
            columns_to_use = list(df.columns)
            variable_type_mapping = st.session_state['variable_type_mapping']

            # Identify categorical columns among the selected variables
            categorical_cols = [col for col in columns_to_use if variable_type_mapping.get(col) == 'categorical']

            if selected_encoding_method == "No Encoding":
                st.write("Skipped encoding of categorical features.")
            elif selected_encoding_method == "One Hot Encoding":
                if categorical_cols:
                    # Apply One Hot Encoding
                    df = pd.get_dummies(df, columns=categorical_cols)
                    st.write("Applied One Hot Encoding.")

                    # Update selected_X with new column names
                    new_columns = df.columns.tolist()
                    original_selected_X = st.session_state['selected_X']
                    # Remove original categorical columns from selected_X and add new one-hot encoded columns
                    st.session_state['selected_X'] = [
                        col for col in new_columns 
                        if col in original_selected_X 
                        or any(col.startswith(f"{orig_col}_") for orig_col in categorical_cols if orig_col in original_selected_X)
                    ]
                else:
                    st.write("No categorical columns available for One Hot Encoding.")
            elif selected_encoding_method == "Ordinal Encoding":
                if categorical_cols:
                    # Apply Ordinal Encoding
                    ordinal_encoder = OrdinalEncoder()
                    df[categorical_cols] = ordinal_encoder.fit_transform(df[categorical_cols])
                    st.write("Applied Ordinal Encoding.")
                else:
                    st.write("No categorical columns available for Ordinal Encoding.")
            elif selected_encoding_method == "Label Encoding":
                if categorical_cols:
                    # Apply Label Encoding
                    label_encoders = {}
                    for col in categorical_cols:
                        le = LabelEncoder()
                        df[col] = le.fit_transform(df[col])
                        label_encoders[col] = le
                    st.write("Applied Label Encoding.")
                else:
                    st.write("No categorical columns available for Label Encoding.")
            elif selected_encoding_method == "Count Frequency Encoding":
                if categorical_cols:
                    # Apply Count Frequency Encoding
                    for col in categorical_cols:
                        freq_encoding = df[col].value_counts()
                        df[col] = df[col].map(freq_encoding)
                    st.write("Applied Count Frequency Encoding.")
                else:
                    st.write("No categorical columns available for Count Frequency Encoding.")
            else:
                st.error("Invalid encoding method selected.")
                st.stop()

            # Update the dataframe in session state
            st.session_state['df'] = df

            st.session_state['step'] = 4
            scroll_to('scaling_continuous_features')

    if st.session_state.get('step', 0) >= 4:
        st.markdown('<div id="scaling_continuous_features"></div>', unsafe_allow_html=True)
        st.subheader("ğŸ« Scaling Continuous Features")
        
        st.markdown("""
        > **Standard Scaling?** \\
        > ë°ì´í„°ì˜ í‰ê· ì„ 0, í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ì¡°ì •í•˜ì—¬ ì •ê·œ ë¶„í¬ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤. \\
        > **Min-Max Scaling?**\\
        > ë°ì´í„°ì˜ ìµœì†Ÿê°’ê³¼ ìµœëŒ“ê°’ì„ 0ê³¼ 1 ì‚¬ì´ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë“  ê°’ì´ ë™ì¼í•œ ë²”ìœ„ì— ìœ„ì¹˜í•˜ë„ë¡ í•©ë‹ˆë‹¤.\\
        > **Robust Scaling?** \\
        > ë°ì´í„°ì˜ ì¤‘ì•™ê°’ê³¼ ì‚¬ë¶„ìœ„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ì˜ ì˜í–¥ì„ ìµœì†Œí™”í•˜ë©´ì„œ ìŠ¤ì¼€ì¼ë§í•©ë‹ˆë‹¤. \\
        > **Max Absolute Scaling?** \\
        > ë°ì´í„°ì˜ ì ˆëŒ“ê°’ ì¤‘ ìµœëŒ“ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  ê°’ì„ -1ì—ì„œ 1 ì‚¬ì´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """)

        # Options for scaling methods, including "No Scaling"
        scaling_options = [
            "Standard Scaling",
            "No Scaling",
            "Min-Max Scaling",
            "Robust Scaling",
            "Max Absolute Scaling"
        ]

        # User selection
        selected_scaling_method = st.selectbox(
            "Select scaling method for continuous features:",
            scaling_options
        )

        # Apply button
        if st.button("Apply Scaling"):
            df = st.session_state['df']
            columns_to_use = list(df.columns)
            variable_type_mapping = st.session_state['variable_type_mapping']

            # Identify continuous columns among the selected variables
            continuous_cols = [col for col in columns_to_use if variable_type_mapping.get(col) == 'continuous']

            if selected_scaling_method == "No Scaling":
                st.write("Skipped scaling of continuous features.")
            elif selected_scaling_method == "Min-Max Scaling":
                if continuous_cols:
                    # Apply Min-Max Scaling
                    scaler = MinMaxScaler()
                    df[continuous_cols] = scaler.fit_transform(df[continuous_cols])
                    st.write("Applied Min-Max Scaling.")
                else:
                    st.write("No continuous columns available for Min-Max Scaling.")
            elif selected_scaling_method == "Standard Scaling":
                if continuous_cols:
                    # Apply Standard Scaling
                    scaler = StandardScaler()
                    df[continuous_cols] = scaler.fit_transform(df[continuous_cols])
                    st.write("Applied Standard Scaling.")
                else:
                    st.write("No continuous columns available for Standard Scaling.")
            elif selected_scaling_method == "Robust Scaling":
                if continuous_cols:
                    # Apply Robust Scaling
                    scaler = RobustScaler()
                    df[continuous_cols] = scaler.fit_transform(df[continuous_cols])
                    st.write("Applied Robust Scaling.")
                else:
                    st.write("No continuous columns available for Robust Scaling.")
            elif selected_scaling_method == "Max Absolute Scaling":
                if continuous_cols:
                    # Apply Max Absolute Scaling
                    scaler = MaxAbsScaler()
                    df[continuous_cols] = scaler.fit_transform(df[continuous_cols])
                    st.write("Applied Max Absolute Scaling.")
                else:
                    st.write("No continuous columns available for Max Absolute Scaling.")
            else:
                st.error("Invalid scaling method selected.")
                st.stop()

            # Update the dataframe in session state
            st.session_state['df'] = df

            st.session_state['step'] = 5
            scroll_to('dataset_splitting')

    if st.session_state.get('step', 0) >= 5:
        st.markdown('<div id="dataset_splitting"></div>', unsafe_allow_html=True)
        st.subheader("ğŸ° Dataset Splitting")

        # Options for splitting
        test_size = st.slider(
            "Select test set size (as a fraction):",
            min_value=0.1, max_value=0.5, value=0.2, step=0.05
        )
        random_state = st.number_input(
            "Set random state for reproducibility:",
            min_value=0, value=42, step=1
        )

        # Apply button
        if st.button("Apply Splitting"):
            df = st.session_state['df']
            selected_X = st.session_state['selected_X']
            selected_Y = st.session_state['selected_Y']

            # Extract features and target variables
            X = df[selected_X]

            # Ensure selected_Y is valid
            if not selected_Y or selected_Y[0] not in df.columns:
                st.error("The selected target variable is not valid or does not exist in the DataFrame.")
                st.stop()  # Stop further execution if the condition is not met

            y = df[selected_Y[0]]  # Assuming single target variable

            # Debugging: Check original values in y
            st.write("Original values in target variable:")
            st.write(y.value_counts())  # Display counts of each class

            # No conversion needed for the target variable

            # Debugging: Check unique values in transformed y
            unique_classes = np.unique(y)
            st.write(f"Unique classes in target variable after transformation: {unique_classes}")  # Debugging statement

            # Check if there are at least 2 classes
            if len(unique_classes) < 2:
                st.error("The target variable must contain at least two classes for classification.")
                st.stop()  # Stop further execution if the condition is not met
            else:
                # Perform the split
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=test_size, random_state=random_state
                )

                # Check the distribution of the target variable in the training set
                st.write("Training set target variable distribution:")
                st.write(pd.Series(y_train).value_counts())  # Display counts of each class

                # Store the splits in session state for later use
                st.session_state['X_train'] = X_train
                st.session_state['X_test'] = X_test
                st.session_state['y_train'] = y_train
                st.session_state['y_test'] = y_test

                st.write("Training set shape:", X_train.shape)
                st.write("Testing set shape:", X_test.shape)

                st.session_state['step'] = 6
                scroll_to('building_model')

    if st.session_state.get('step', 0) >= 6:
        st.markdown('<div id="building_model"></div>', unsafe_allow_html=True)
        st.subheader("ğŸ‘¾ Building Model")
        
        st.markdown("""
        > **Logistic Regression?** \\
        > ë°ì´í„°ë¥¼ ì§ì„ ìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ì„ í˜• ë¶„ë¥˜ ëª¨ë¸ë¡œ, ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ íŠ¹ì • í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤. \\
        > **Decision Tree?**\\
        > ë°ì´í„°ë¥¼ ì¡°ê±´ì— ë”°ë¼ ë¶„ë¦¬í•˜ë©° ì˜ì‚¬ê²°ì • ê²½ë¡œë¥¼ ìƒì„±í•˜ëŠ” ë¹„ì„ í˜• ë¶„ë¥˜ ëª¨ë¸ë¡œ, ë°ì´í„° ë¶„í¬ì™€ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.\\
        > **Random Forest?** \\
        > ì—¬ëŸ¬ ê°œì˜ ê²°ì • íŠ¸ë¦¬ë¥¼ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì„ ì¢…í•©í•˜ëŠ” ì•™ìƒë¸” ëª¨ë¸ë¡œ, ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. \\
        > **Support Vector Classifier?** \\
        > ë°ì´í„°ë¥¼ ìµœëŒ€ í­ì˜ ê²½ê³„ë¡œ ë¶„ë¦¬í•˜ëŠ” ì´ˆí‰ë©´ì„ ì°¾ëŠ” ë¹„ì„ í˜• ë¶„ë¥˜ ëª¨ë¸ë¡œ, ê³ ì°¨ì› ë°ì´í„°ì—ì„œë„ íš¨ê³¼ì ì…ë‹ˆë‹¤.
        """)

        # ì„¸ì…˜ ìƒíƒœì—ì„œ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        X_train = st.session_state['X_train']
        y_train = st.session_state['y_train']
        X_test = st.session_state['X_test']
        y_test = st.session_state['y_test']
        selected_Y = st.session_state['selected_Y']
        column_mapping = st.session_state['column_mapping']

        # ëª¨ë¸ ìœ í˜• ì„ íƒ ì˜µì…˜ ì œê³µ (í˜„ì¬ Classificationë§Œ ì‚¬ìš©)
        model_type = st.selectbox("Select Model Type:", ['Classification'])

        # ëª¨ë¸ ì„ íƒ ì˜µì…˜ ì¶”ê°€
        selected_model = st.selectbox("Select Model:", ['Random Forest Classifier', 'Logistic Regression', 'Decision Tree Classifier', 'Support Vector Classifier'])

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì„ íƒ ë²„íŠ¼ ì¶”ê°€
        hyperparameter_tuning = st.checkbox("Enable Hyperparameter Tuning - â° 3min")

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ í™œì„±í™”ëœ ê²½ìš°
        if hyperparameter_tuning:
            st.markdown("""
            > **Halving Grid Search?** \\
            > ì „ì²´ ë§¤ê°œë³€ìˆ˜ ì¡°í•© ì¤‘ ì¼ë¶€ë¥¼ ì ì§„ì ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ë¦¬ì†ŒìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ì„œ ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì°¾ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. \\
            > **Grid Search?**\\
            > ëª¨ë“  ë§¤ê°œë³€ìˆ˜ ì¡°í•©ì„ ì²´ê³„ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì°¾ëŠ” ë°©ë²•ìœ¼ë¡œ, ê³„ì‚° ì‹œê°„ì´ ë§ì´ ì†Œìš”ë©ë‹ˆë‹¤.\\
            > **Randomized Search?** \\
            > ë§¤ê°œë³€ìˆ˜ ê³µê°„ì—ì„œ ëœë¤í•˜ê²Œ ì¼ë¶€ ì¡°í•©ì„ ì„ íƒí•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ íƒìƒ‰í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
            """)
            tuning_method = st.selectbox("Select Hyperparameter Tuning Method:", ["Halving Grid Search", "Grid Search", "Randomized Search"])
            k_folds = st.number_input("Select number of folds for cross-validation (k):", min_value=2, value=5, step=1)
        else:
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„í™œì„±í™” ì‹œ, ì‚¬ìš©ìê°€ ì§ì ‘ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
            st.write("Manually set hyperparameters:")

            if selected_model == 'Logistic Regression':
                C_value = st.number_input("C (Inverse of regularization strength):", min_value=0.0001, value=1.0, step=0.1)
                penalty = st.selectbox("Penalty:", ['l1', 'l2'])
                solver = st.selectbox("Solver:", ['liblinear', 'saga'])
            elif selected_model == 'Decision Tree Classifier':
                max_depth = st.selectbox("Max Depth:", [None, 5, 10])
                min_samples_split = st.selectbox("Min Samples Split:", [2, 5, 10])
                min_samples_leaf = st.selectbox("Min Samples Leaf:", [1, 2, 4])
            elif selected_model == 'Random Forest Classifier':
                n_estimators = st.selectbox("Number of Estimators:", [50, 100, 200])
                max_depth = st.selectbox("Max Depth:", [None, 5, 10])
                min_samples_split = st.selectbox("Min Samples Split:", [2, 5, 10])
                min_samples_leaf = st.selectbox("Min Samples Leaf:", [1, 2, 4])
            elif selected_model == 'Support Vector Classifier':
                C_value = st.selectbox("C (Regularization parameter):", [0.1, 1, 10])
                kernel = st.selectbox("Kernel:", ['linear', 'rbf', 'poly'])
                gamma = st.selectbox("Gamma:", ['scale', 'auto'])

        # Train Model ë²„íŠ¼ ì¶”ê°€
        if st.button("Train Model"):
            st.write("Training model...")  # Debugging statement

            # íŠœë‹ìš© íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
            if selected_model == 'Logistic Regression':
                model = LogisticRegression(max_iter=1000)
                param_grid = {
                    'C': [0.01, 0.1, 1, 10],
                    'penalty': ['l1', 'l2'],
                    'solver': ['liblinear', 'saga']
                }
            elif selected_model == 'Decision Tree Classifier':
                model = DecisionTreeClassifier(random_state=42)
                param_grid = {
                    'max_depth': [None, 5, 10],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                }
            elif selected_model == 'Random Forest Classifier':
                model = RandomForestClassifier(random_state=42)
                param_grid = {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [None, 5, 10],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                }
            elif selected_model == 'Support Vector Classifier':
                model = SVC(probability=True)
                param_grid = {
                    'C': [0.1, 1, 10],
                    'kernel': ['linear', 'rbf', 'poly'],
                    'gamma': ['scale', 'auto']
                }

            # ì§„í–‰ ìƒí™© í‘œì‹œ ë°” ì´ˆê¸°í™”
            progress_bar = st.progress(0)
            status_text = st.empty()  # ìƒíƒœ í‘œì‹œ í…ìŠ¤íŠ¸    

            if hyperparameter_tuning:
                st.write("Hyperparameter tuning in progress...")

                if tuning_method == "Grid Search":
                    search_cv = GridSearchCV(model, param_grid, cv=k_folds, n_jobs=-1, scoring='f1_macro')
                elif tuning_method == "Randomized Search":
                    search_cv = RandomizedSearchCV(model, param_grid, cv=k_folds, n_jobs=-1, scoring='f1_macro', n_iter=10, random_state=42)
                elif tuning_method == "Halving Grid Search":
                    search_cv = HalvingGridSearchCV(model, param_grid, cv=k_folds, scoring='f1_macro', factor=2, random_state=42)

                for percent_complete in range(0, 101, 10):  # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ (ê°€ìƒ ì‹œë®¬ë ˆì´ì…˜)
                    progress_bar.progress(percent_complete / 100)
                    status_text.write(f"Preparing... {percent_complete}%")
                    sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•´ ë”œë ˆì´

                search_cv.fit(X_train, y_train)
                best_model = search_cv.best_estimator_
                st.write("Best Parameters:", search_cv.best_params_)
                model = best_model
            else:
                # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¹„í™œì„±í™” ì‹œ, ì‚¬ìš©ìê°€ ì„¤ì •í•œ ê°’ìœ¼ë¡œ ëª¨ë¸ ì„¤ì •
                if selected_model == 'Logistic Regression':
                    model = LogisticRegression(C=C_value, penalty=penalty, solver=solver, max_iter=1000)
                elif selected_model == 'Decision Tree Classifier':
                    model = DecisionTreeClassifier(random_state=42, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)
                elif selected_model == 'Random Forest Classifier':
                    model = RandomForestClassifier(random_state=42, n_estimators=n_estimators, max_depth=max_depth, 
                                                   min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)
                elif selected_model == 'Support Vector Classifier':
                    model = SVC(probability=True, C=C_value, kernel=kernel, gamma=gamma)

                # ëª¨ë¸ í•™ìŠµ ì§„í–‰ ìƒí™© í‘œì‹œ (ê°€ìƒ ì‹œë®¬ë ˆì´ì…˜)
                for percent_complete in range(0, 101, 20):
                    progress_bar.progress(percent_complete / 100)
                    status_text.write(f"Training Progress: {percent_complete}%")
                    sleep(0.2)  # ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•´ ë”œë ˆì´

                model.fit(X_train, y_train)

            # í•™ìŠµ ì™„ë£Œ í›„ ì§„í–‰ ìƒí™© ë°”ì™€ ìƒíƒœ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            progress_bar.progress(1.0)
            status_text.write("Training Complete!")

            st.session_state['model'] = model  # Store the trained model in session state
            st.success(f"{selected_model} model trained successfully.")

    if st.session_state.get('step', 0) >= 6:
        st.markdown('<div id="visualization_evaluation"></div>', unsafe_allow_html=True)
        st.subheader("ğŸ¨ Visualization & Evaluation")

        if 'model' in st.session_state:
            # ëª¨ë¸ ì˜ˆì¸¡
            y_pred = st.session_state['model'].predict(st.session_state['X_test'])

            # Check if predictions are valid
            if y_pred is not None and len(y_pred) > 0:
                # í‰ê°€ ì§€í‘œ ê³„ì‚°
                accuracy = accuracy_score(st.session_state['y_test'], y_pred)
                f1_macro = f1_score(st.session_state['y_test'], y_pred, average='macro')

                # Feature Importance ê³„ì‚°
                model = st.session_state['model']
                X_train = st.session_state['X_train']  # X_train ë¶ˆëŸ¬ì˜¤ê¸°
                selected_features = X_train.columns
                df_imp = None

                if hasattr(model, 'feature_importances_'):
                    importances = model.feature_importances_
                    df_imp = pd.DataFrame({
                        'Feature': selected_features,
                        'Importance': importances
                    })
                elif hasattr(model, 'coef_'):
                    coefs = model.coef_
                    if coefs.ndim > 1:
                        coefs = np.mean(np.abs(coefs), axis=0)
                    else:
                        coefs = np.abs(coefs.flatten())
                    df_imp = pd.DataFrame({
                        'Feature': selected_features,
                        'Importance': coefs
                    })
                else:
                    # feature importanceë¥¼ ì œê³µí•˜ì§€ ì•ŠëŠ” ëª¨ë¸
                    df_imp = pd.DataFrame({
                        'Feature': [],
                        'Importance': []
                    })
                    st.write("This model does not provide feature importances. Top features not available.")

                if not df_imp.empty:
                    df_imp['Feature_Label'] = df_imp['Feature'].apply(lambda f: column_mapping.get(f, f))
                    df_imp = df_imp.sort_values('Importance', ascending=False)

                gradient_style = """
                <style>
                .gradient-text {
                    font-size: 25px;
                    font-weight: bold;
                    background: linear-gradient(90deg, #ff7e5f, #feb47b, #86a8e7, #91eae4);
                    -webkit-background-clip: text;
                    -webkit-text-fill-color: transparent;
                    text-align: center;
                    margin: 0;
                    padding: 0;
                }
                </style>
                """

                st.markdown(gradient_style, unsafe_allow_html=True)

                # 1í–‰: Accuracy, F1
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown(f"<p class='gradient-text'>Model Accuracy: {accuracy:.2f}</p>", unsafe_allow_html=True)
                with col2:
                    st.markdown(f"<p class='gradient-text'>Macro Average F1-Score: {f1_macro:.2f}</p>", unsafe_allow_html=True)

                st.markdown('\n')

                # 2í–‰: Feature Importances ìƒìœ„ 3ê°œ (ë§Œì•½ df_impê°€ ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´)
                if not df_imp.empty:
                    top_3_features = df_imp['Feature_Label'].head(3).tolist()
                    top_features_text = " & ".join(top_3_features)
                    st.markdown(f"<p class='gradient-text'>{top_features_text}</p>", unsafe_allow_html=True)

                st.markdown('\n')

                # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” ì¶”ê°€
                if not df_imp.empty:
                    fig_importance = px.bar(
                        df_imp.head(20),  # ìƒìœ„ 20ê°œ íŠ¹ì„± í‘œì‹œ (í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
                        x='Importance',
                        y='Feature_Label',
                        orientation='h',
                        title='Feature Importance',
                        labels={'Importance': 'Importance Score', 'Feature_Label': 'Feature'},
                        color='Importance',
                        color_continuous_scale='Viridis'
                    )
                    fig_importance.update_layout(
                        yaxis={'categoryorder': 'total ascending'},  # ì¤‘ìš”ë„ì— ë”°ë¼ ì •ë ¬
                        margin=dict(l=100, r=20, t=50, b=20)
                    )
                    st.plotly_chart(fig_importance, use_container_width=True)

                # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
                unique_labels = np.unique(st.session_state['y_test'])
                conf_matrix = confusion_matrix(st.session_state['y_test'], y_pred, labels=unique_labels)
                fig = px.imshow(conf_matrix,
                                labels=dict(x="Predicted", y="True", color="Count"),
                                x=[f'Predicted {cls}' for cls in unique_labels],
                                y=[f'True {cls}' for cls in unique_labels],
                                text_auto=True,
                                color_continuous_scale='Blues')
                fig.update_layout(title='Confusion Matrix', xaxis_title='Predicted', yaxis_title='True')
                st.plotly_chart(fig)

                # ë¶„ë¥˜ ë¦¬í¬íŠ¸
                report = classification_report(st.session_state['y_test'], y_pred, output_dict=True)
                st.write("**Classification Report:**")
                st.dataframe(pd.DataFrame(report).transpose())

                # ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ë¹„êµ ê·¸ë˜í”„
                st.write('\n')
                st.write("**Actual vs Predicted Counts**")
                actual_counts = pd.Series(st.session_state['y_test'], name='Actual').value_counts().sort_index()
                predicted_counts = pd.Series(y_pred, name='Predicted').value_counts().sort_index()

                all_labels = np.unique(np.concatenate((unique_labels, np.unique(y_pred))))
                df_compare = pd.DataFrame({
                    'Class': all_labels,
                    'Actual': [actual_counts[cls] if cls in actual_counts.index else 0 for cls in all_labels],
                    'Predicted': [predicted_counts[cls] if cls in predicted_counts.index else 0 for cls in all_labels]
                })

                fig_compare = px.bar(df_compare, 
                                     x='Class', 
                                     y=['Actual', 'Predicted'], 
                                     barmode='group', 
                                     title=False,
                                     labels={'value': 'Count', 'Class':'Class'},
                                     text_auto=True,
                                     color_discrete_sequence=px.colors.qualitative.Pastel)

                fig_compare.update_layout(
                    # title_font_size=20,
                    # xaxis_title_font_size=16,
                    # yaxis_title_font_size=16,
                    legend_title_text='',
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=1.02,
                        xanchor="right",
                        x=1
                    )
                )
                st.plotly_chart(fig_compare, use_container_width=True)

            else:
                st.error("No predictions were made. Please check the model training.")
        else:
            st.error("Model has not been trained yet. Please train the model first.")